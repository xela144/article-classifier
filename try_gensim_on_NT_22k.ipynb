{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from db import psqlServer\n",
    "sv = psqlServer()\n",
    "\n",
    "control_chars = [chr(0x85)]\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"arx_AI\"        # artificial intelligence\n",
    "FA = \"arx_math_FA\"   # functional analysis\n",
    "GR = \"arx_GRQC\"      # general relativity and quantum cosmology\n",
    "LG = \"arx\"           # learning\n",
    "NT = \"arx_math_NT\"   # number theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116060\n"
     ]
    }
   ],
   "source": [
    "# How many items in all tables?\n",
    "UNION = \" UNION ALL \"\n",
    "SEL_C = \"SELECT COUNT(*) as FOO FROM \"\n",
    "sum_statement = \"SELECT SUM(FOO) FROM (\"\n",
    "sum_statement += SEL_C + AI\n",
    "sum_statement += UNION + SEL_C + FA \n",
    "sum_statement += UNION + SEL_C + GR \n",
    "sum_statement += UNION + SEL_C + LG \n",
    "sum_statement += UNION + SEL_C + NT\n",
    "sum_statement += \") as B;\"\n",
    "\n",
    "total = sv.execute(sum_statement)[0]['sum']\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:13000\n",
      "FA:16000\n",
      "GR:50000\n",
      "LG:15060\n",
      "NT:22000\n"
     ]
    }
   ],
   "source": [
    "# Per table\n",
    "SEL_C = \"SELECT COUNT(*) FROM \"\n",
    "total_AI = sv.execute(SEL_C + AI)[0]['count']\n",
    "total_FA = sv.execute(SEL_C + FA)[0]['count']\n",
    "total_GR = sv.execute(SEL_C + GR)[0]['count']\n",
    "total_LG = sv.execute(SEL_C + LG)[0]['count']\n",
    "total_NT = sv.execute(SEL_C + NT)[0]['count']\n",
    "\n",
    "print(\"AI:{}\".format(total_AI))\n",
    "print(\"FA:{}\".format(total_FA))\n",
    "print(\"GR:{}\".format(total_GR))\n",
    "print(\"LG:{}\".format(total_LG))\n",
    "print(\"NT:{}\".format(total_NT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some rows from the AI table\n",
    "rows = sv.execute(\"SELECT * FROM arx_math_NT\")\n",
    "corpus = [x['abstract'] for x in rows]\n",
    "labels = [float(x['has_journal_ref']) for x in rows]\n",
    "\n",
    "#rows2 = sv.execute(\"SELECT abstract FROM arx_math_FA LIMIT 10000\")\n",
    "#corpus += [x['abstract'] for x in rows2]\n",
    "#labels += [x['has_journal_ref'] for x in rows2]\n",
    "\n",
    "#rows3 = sv.execute(\"SELECT abstract FROM arx_GRQC LIMIT 30000\")\n",
    "#corpus += [x['abstract'] for x in rows3]\n",
    "#labels += [x['has_journal_ref'] for x in rows2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_n = []\n",
    "jj = 0\n",
    "for corp in corpus:\n",
    "    corpus_n.append(u\"_*{0} {1}\\n\".format(jj, normalize_text(corp)))\n",
    "    jj += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "alldocs = []\n",
    "for line_no, line in enumerate(corpus_n[0:5500]):\n",
    "    tokens = gensim.utils.to_unicode(corpus_n[line_no]).split()\n",
    "    words = tokens[1:]\n",
    "    tags = [line_no]\n",
    "    split = 'train'\n",
    "    #split = ['train', 'test', 'extra', 'extra'][line_no//12500]\n",
    "    sentiment = labels[line_no]\n",
    "    #sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//6750]\n",
    "    alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "for line_no, line in enumerate(corpus_n[5500:11000]):\n",
    "    tokens = gensim.utils.to_unicode(corpus_n[line_no]).split()\n",
    "    words = tokens[1:]\n",
    "    tags = [line_no]\n",
    "    split = 'test'\n",
    "    #split = ['train', 'test', 'extra', 'extra'][line_no//12500]\n",
    "    sentiment = labels[line_no]\n",
    "    #sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//6750]\n",
    "    alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "    \n",
    "for line_no, line in enumerate(corpus_n[11000:]):\n",
    "    tokens = gensim.utils.to_unicode(corpus_n[line_no]).split()\n",
    "    words = tokens[1:]\n",
    "    tags = [line_no]\n",
    "    split = 'extra'\n",
    "    #split = ['train', 'test', 'extra', 'extra'][line_no//12500]\n",
    "    sentiment = None\n",
    "    #sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//6750]\n",
    "    alldocs.append(SentimentDocument(words, tags, split, sentiment))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000 docs: 5500 train-sentiment, 5500 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # For reshuffling per pass\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_models[0].build_vocab(alldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    \"\"\"Reuse shareable structures from other_model.\"\"\"\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x10d2410b8>),\n",
       "             ('Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x10d241198>),\n",
       "             ('Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)',\n",
       "              <gensim.models.doc2vec.Doc2Vec at 0x10d241240>),\n",
       "             ('dbow+dmm',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x1144ca240>),\n",
       "             ('dbow+dmc',\n",
       "              <gensim.test.test_doc2vec.ConcatenatedDoc2Vec at 0x1144ca208>)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/code/article-classifier/ac/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictive Evaluation Methods\n",
    "\n",
    "# Let's define some helper methods for evaluating the performance of our Doc2vec\n",
    "# using paragraph vectors. We will classify document sentiments using a logistic \n",
    "# regression model based on our paragraph embeddings. We will compare the error \n",
    "# rates based on word embeddings from our various Doc2vec models.\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# For timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    \n",
    "    # Here is the wtf: test_data is just [None]*500!!!\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bulk Training\n",
    "\n",
    "# We use an explicit multiple-pass, alpha-reduction approach as sketched in \n",
    "# this gensim doc2vec blog post with added shuffling of corpus on each pass.\n",
    "# https://rare-technologies.com/doc2vec-tutorial/\n",
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-11-30 23:33:52.614466\n",
      "*0.283091 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.2s 0.2s\n",
      "*0.314545 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 3.2s 0.8s\n",
      "*0.281091 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.6s 0.2s\n",
      "*0.370909 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 1.6s 0.4s\n",
      "*0.280909 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.7s 0.2s\n",
      "*0.292727 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 2.7s 0.5s\n",
      "*0.273818 : 1 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.341818 : 1 passes : dbow+dmm_inferred 0.0s 0.9s\n",
      "*0.275636 : 1 passes : dbow+dmc 0.0s 0.4s\n",
      "*0.329091 : 1 passes : dbow+dmc_inferred 0.0s 1.2s\n",
      "Completed pass 1 at alpha 0.025000\n",
      "*0.282000 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.1s 0.2s\n",
      "*0.278182 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.5s 0.2s\n",
      "*0.280909 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.8s 0.2s\n",
      "*0.272182 : 2 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.270182 : 2 passes : dbow+dmc 0.0s 0.4s\n",
      "Completed pass 2 at alpha 0.022600\n",
      "*0.279455 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.0s 0.2s\n",
      "*0.276182 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.5s 0.2s\n",
      " 0.281455 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.8s 0.2s\n",
      "*0.271818 : 3 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.266727 : 3 passes : dbow+dmc 0.0s 0.4s\n",
      "Completed pass 3 at alpha 0.020200\n",
      "*0.279273 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.1s 0.2s\n",
      " 0.276909 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 2.1s 0.5s\n",
      "*0.279091 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 3.7s 0.2s\n",
      " 0.273455 : 4 passes : dbow+dmm 0.0s 0.5s\n",
      " 0.270364 : 4 passes : dbow+dmc 0.0s 0.5s\n",
      "Completed pass 4 at alpha 0.017800\n",
      "*0.278000 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.3s 0.2s\n",
      "*0.296364 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 3.3s 0.7s\n",
      " 0.277455 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.6s 0.2s\n",
      "*0.261818 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 1.6s 0.4s\n",
      "*0.278545 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.8s 0.2s\n",
      " 0.301818 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 2.8s 0.5s\n",
      "*0.270727 : 5 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.314545 : 5 passes : dbow+dmm_inferred 0.0s 0.9s\n",
      " 0.268727 : 5 passes : dbow+dmc 0.0s 0.4s\n",
      "*0.318182 : 5 passes : dbow+dmc_inferred 0.0s 1.2s\n",
      "Completed pass 5 at alpha 0.015400\n",
      " 0.279818 : 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.0s 0.2s\n",
      " 0.278364 : 6 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.6s 0.2s\n",
      " 0.279455 : 6 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.8s 0.2s\n",
      "*0.267273 : 6 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.266364 : 6 passes : dbow+dmc 0.0s 0.4s\n",
      "Completed pass 6 at alpha 0.013000\n",
      " 0.281455 : 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 2.9s 0.2s\n",
      "*0.276000 : 7 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.6s 0.2s\n",
      " 0.280000 : 7 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 2.8s 0.2s\n",
      " 0.267636 : 7 passes : dbow+dmm 0.0s 0.4s\n",
      " 0.269091 : 7 passes : dbow+dmc 0.0s 0.4s\n",
      "Completed pass 7 at alpha 0.010600\n",
      " 0.281818 : 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.0s 0.2s\n",
      " 0.276545 : 8 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.6s 0.2s\n",
      " 0.279818 : 8 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 3.0s 0.2s\n",
      " 0.268727 : 8 passes : dbow+dmm 0.0s 0.4s\n",
      " 0.268364 : 8 passes : dbow+dmc 0.0s 0.4s\n",
      "Completed pass 8 at alpha 0.008200\n",
      " 0.281818 : 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 3.2s 0.2s\n",
      " 0.276909 : 9 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 1.8s 0.2s\n",
      " 0.280000 : 9 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 3.9s 0.2s\n",
      "*0.267091 : 9 passes : dbow+dmm 0.0s 0.7s\n",
      " 0.267455 : 9 passes : dbow+dmc 0.0s 0.6s\n",
      "Completed pass 9 at alpha 0.005800\n",
      " 0.281455 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8) 4.2s 0.2s\n",
      "*0.281818 : 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred 4.2s 0.8s\n",
      " 0.276364 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8) 2.5s 0.3s\n",
      " 0.329091 : 10 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred 2.5s 0.7s\n",
      " 0.279818 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8) 3.2s 0.2s\n",
      "*0.265455 : 10 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred 3.2s 0.5s\n",
      " 0.268000 : 10 passes : dbow+dmm 0.0s 0.4s\n",
      "*0.312727 : 10 passes : dbow+dmm_inferred 0.0s 1.3s\n",
      " 0.266727 : 10 passes : dbow+dmc 0.0s 0.4s\n",
      "*0.303636 : 10 passes : dbow+dmc_inferred 0.0s 1.6s\n",
      "Completed pass 10 at alpha 0.003400\n",
      "END 2017-11-30 23:35:40.320728\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 10)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err rate Model\n",
      "0.261818 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)_inferred\n",
      "0.265455 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)_inferred\n",
      "0.266364 dbow+dmc\n",
      "0.267091 dbow+dmm\n",
      "0.276000 Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "0.278000 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "0.278545 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
      "0.281818 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)_inferred\n",
      "0.303636 dbow+dmc_inferred\n",
      "0.312727 dbow+dmm_inferred\n"
     ]
    }
   ],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 9541...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      " [(990, 0.7450541257858276), (6155, 0.7244516015052795), (2140, 0.717074453830719)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      " [(4041, 0.8820165991783142), (8378, 0.6651365756988525), (8939, 0.6619388461112976)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8):\n",
      " [(4041, 0.7913916707038879), (4551, 0.764366865158081), (8779, 0.7562659978866577)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (6744): «we study the free path length and the geometric free path length in the model of the periodic two-dimensional lorentz gas ( sinai billiard ) . we give a complete and rigorous proof for the existence of their distributions in the small-scatterer limit and explicitly compute them . as a corollary one gets a complete proof for the existence of the constant term $c=2-3\\ln 2+\\frac{27\\zeta ( 3 ) }{2\\pi^2}$ in the asymptotic formula $h ( t ) =-2\\ln \\eps+c+o ( 1 ) $ of the ks entropy of the billiard map in this model , as conjectured by p . dahlqvist .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (9502, 0.7147164344787598): «for certain real quadratic number fields , we prove density results concerning 4-ranks of tame kernels . we also discuss a relationship between 4-ranks of tame kernels and 4-class ranks of narrow ideal class groups . additionally , we give a product formula for a local hilbert symbol .»\n",
      "\n",
      "MEDIAN (2286, 0.396057665348053): «in this paper , we study the relation between the zeta function of a calabi-yau hypersurface and the zeta function of its mirror . two types of arithmetic relations are discovered . this motivates us to formulate two general arithmetic mirror conjectures for the zeta functions of a mirror pair of calabi-yau manifolds .»\n",
      "\n",
      "LEAST (635, 0.051576174795627594): «let f : x -> y be a separated morphism of schemes of finite type over a finite field of characteristic p , let lambda be an artinian local z_p-algebra with finite residue field , let m be the maximal ideal of lambda , and let l^\\bullet be a bounded constructible complex of sheaves of finite free lambda-modules on the \\'etale site of y . we show that the ratio of l-functions l ( x , l^\\bullet ) /l ( y , f_ ! l^\\bullet ) , which is a priori an element of 1+t lambda[[t]] , in fact lies in 1+ m t lambda [t] . this implies a conjecture of katz predicting the location of the zeroes and poles of the l-function of a p-adic \\'etale lisse sheaf on the closed unit disk in terms of \\'etale cohomology with compact support .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (4027): «we generalize the farey-brocot partition to a twodimensional continued fraction algorithm and generalized farey-brocot nets . we give an asymptotic formula for the moments of order \\beta .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (5883, 0.8089919686317444): «let $p ( z ) $ be a monic polynomial of degree $n$ , with complex coefficients , and let $q ( z ) $ be its monic factor . we prove an asymptotically sharp inequality of the form $\\|q\\|_{e} \\le c^n \\|p\\|_e$ , where $\\|\\cdot\\|_e$ denotes the sup norm on a compact set $e$ in the plane . the best constant $c_e$ in this inequality is found by potential theoretic methods . we also consider applications of the general result to the cases of a disk and a segment .»\n",
      "\n",
      "MEDIAN (7354, 0.0511329285800457): «we consider the primes which divide the denominator of the x-coordinate of a sequence of rational points on an elliptic curve . it is expected that for every sufficiently large value of the index , each term should be divisible by a primitive prime divisor , one that has not appeared in any earlier term . proofs of this are known in only a few cases . weaker results in the general direction are given , using a strong form of siegel's theorem and some congruence arguments . our main result is applied to the study of prime divisors of somos sequences .»\n",
      "\n",
      "LEAST (6514, -0.7802834510803223): «this paper gives a brief description of the author's database of integer sequences , now over 35 years old , together with a selection of a few of the most interesting sequences in the table . many unsolved problems are mentioned .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (1168): «we begin with a review of the structure of simple , simply-connected complex lie groups and their lie algebras , describe the chevalley lattice and the associated split group over the integers . this gives us a hyperspecial maximal compact subgroup of the p-adic lie group and we describe the other maximal parahoric subgroups and their lie algebras starting from the hyperspecial one . we then consider the killing form on the chevalley lattice and show that it is divisible by 2 times the dual coxeter number . the same holds for the lie algebras of the other maximal parahorics . we compute the discriminants of the resulting scaled forms . finally we consider jordan subgroups of the exceptional groups . we show that these jordan subgroups are globally maximal and determine their maximal compact overgroups in the p-adic lie group . the last section treats the jordan subgroups of the classical groups .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8):\n",
      "\n",
      "MOST (1659, 0.7445718050003052): «the cuspidal cohomology groups of arithmetic groups in certain infinite dimensional modules are computed . as a result we get a simultaneous generalization of the patterson-conjecture and the lewis-correspondence .»\n",
      "\n",
      "MEDIAN (884, 0.03191511705517769): «we study spin-1/2 heisenberg xxx antiferromagnet . the spectrum of the hamiltonian was found by hans bethe in 1931 . we study the probability of formation of ferromagnetic string in the antiferromagnetic ground state , which we call emptiness formation probability p ( n ) . this is the most fundamental correlation function . we prove that for the short strings it can be expressed in terms of the riemann zeta function with odd arguments , logarithm ln 2 and rational coefficients . this adds yet another link between statistical mechanics and number theory . we have obtained an analytical formula for p ( 5 ) for the first time . we have also calculated p ( n ) numerically by the density matrix renormalization group . the results agree quite well with the analytical ones . furthermore we study asymptotic behavior of p ( n ) at finite temperature by quantum monte-carlo simulation . it also agrees with our previous analytical results .»\n",
      "\n",
      "LEAST (10235, -0.7167800664901733): «in 1999 berry and keating showed that a regularization of the 1d classical hamiltonian h = xp gives semiclassically the smooth counting function of the riemann zeros . in this paper we first generalize this result by considering a phase space delimited by two boundary functions in position and momenta , which induce a fluctuation term in the counting of energy levels . we next quantize the xp hamiltonian , adding an interaction term that depends on two wave functions associated to the classical boundaries in phase space . the general model is solved exactly , obtaining a continuum spectrum with discrete bound states embbeded in it . we find the boundary wave functions , associated to the berry-keating regularization , for which the average riemann zeros become resonances . a spectral realization of the riemann zeros is achieved exploiting the symmetry of the model under the exchange of position and momenta which is related to the duality symmetry of the zeta function . the boundary wave functions , giving rise to the riemann zeros , are found using the riemann-siegel formula of the zeta function . other dirichlet l-functions are shown to find a natural realization in the model .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
