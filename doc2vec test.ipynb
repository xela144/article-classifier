{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
    "\n",
    "import gensim.models as g\n",
    "import logging\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0 #0 = dbow; 1 = dmpv\n",
    "worker_count = 1 #number of parallel processes\n",
    "\n",
    "#pretrained word embeddings\n",
    "pretrained_emb = \"toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
    "\n",
    "#input corpus\n",
    "train_corpus = \"toy_data/train_docs.txt\"\n",
    "\n",
    "#output model\n",
    "saved_path = \"toy_data/model.bin\"\n",
    "\n",
    "#enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.TaggedLineDocument at 0x10c035780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 19:50:51,052 : INFO : collecting all words and their counts\n",
      "2017-11-28 19:50:51,053 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-11-28 19:50:51,090 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2017-11-28 19:50:51,091 : INFO : Loading a fresh vocabulary\n",
      "2017-11-28 19:50:51,130 : INFO : min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2017-11-28 19:50:51,131 : INFO : min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2017-11-28 19:50:51,181 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2017-11-28 19:50:51,182 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2017-11-28 19:50:51,183 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2017-11-28 19:50:51,184 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2017-11-28 19:50:51,227 : INFO : resetting layer weights\n",
      "2017-11-28 19:50:51,404 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2017-11-28 19:50:52,462 : INFO : PROGRESS: at 1.76% examples, 39577 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:53,533 : INFO : PROGRESS: at 3.54% examples, 39442 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:50:54,568 : INFO : PROGRESS: at 5.04% examples, 37859 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:55,629 : INFO : PROGRESS: at 6.71% examples, 37620 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:56,655 : INFO : PROGRESS: at 8.10% examples, 36633 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:57,682 : INFO : PROGRESS: at 9.54% examples, 35991 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:58,760 : INFO : PROGRESS: at 11.14% examples, 35986 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:50:59,786 : INFO : PROGRESS: at 12.71% examples, 35912 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:00,844 : INFO : PROGRESS: at 14.48% examples, 36341 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:01,869 : INFO : PROGRESS: at 16.00% examples, 36203 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:02,908 : INFO : PROGRESS: at 17.54% examples, 36093 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:03,948 : INFO : PROGRESS: at 19.14% examples, 36195 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:04,965 : INFO : PROGRESS: at 20.71% examples, 36146 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:06,021 : INFO : PROGRESS: at 22.36% examples, 36222 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:07,027 : INFO : PROGRESS: at 23.89% examples, 36194 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:08,043 : INFO : PROGRESS: at 25.43% examples, 36189 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:09,063 : INFO : PROGRESS: at 26.96% examples, 36136 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:10,128 : INFO : PROGRESS: at 28.59% examples, 36170 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:11,189 : INFO : PROGRESS: at 30.10% examples, 36048 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:12,258 : INFO : PROGRESS: at 31.89% examples, 36208 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:13,287 : INFO : PROGRESS: at 33.43% examples, 36176 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:14,333 : INFO : PROGRESS: at 35.04% examples, 36216 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:15,348 : INFO : PROGRESS: at 36.83% examples, 36430 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:16,383 : INFO : PROGRESS: at 38.59% examples, 36602 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:17,409 : INFO : PROGRESS: at 40.36% examples, 36767 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:18,428 : INFO : PROGRESS: at 42.10% examples, 36917 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:19,475 : INFO : PROGRESS: at 43.65% examples, 36837 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:20,524 : INFO : PROGRESS: at 45.43% examples, 36959 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:21,525 : INFO : PROGRESS: at 47.14% examples, 37106 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:22,565 : INFO : PROGRESS: at 48.96% examples, 37212 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:23,598 : INFO : PROGRESS: at 50.59% examples, 37232 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:24,679 : INFO : PROGRESS: at 52.23% examples, 37199 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:25,733 : INFO : PROGRESS: at 53.89% examples, 37186 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:26,772 : INFO : PROGRESS: at 55.43% examples, 37129 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:27,811 : INFO : PROGRESS: at 56.96% examples, 37059 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:28,833 : INFO : PROGRESS: at 58.72% examples, 37163 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:29,894 : INFO : PROGRESS: at 60.48% examples, 37237 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:30,917 : INFO : PROGRESS: at 62.23% examples, 37330 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:31,946 : INFO : PROGRESS: at 64.00% examples, 37403 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:32,956 : INFO : PROGRESS: at 65.76% examples, 37502 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:33,971 : INFO : PROGRESS: at 67.54% examples, 37596 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:34,995 : INFO : PROGRESS: at 69.14% examples, 37605 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:36,066 : INFO : PROGRESS: at 70.83% examples, 37580 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:37,078 : INFO : PROGRESS: at 72.36% examples, 37549 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:38,088 : INFO : PROGRESS: at 74.00% examples, 37570 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:39,141 : INFO : PROGRESS: at 75.64% examples, 37562 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:40,155 : INFO : PROGRESS: at 76.96% examples, 37409 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:41,189 : INFO : PROGRESS: at 78.59% examples, 37415 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:42,232 : INFO : PROGRESS: at 80.23% examples, 37416 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:43,285 : INFO : PROGRESS: at 81.89% examples, 37404 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:44,288 : INFO : PROGRESS: at 83.43% examples, 37390 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:45,340 : INFO : PROGRESS: at 85.04% examples, 37382 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:46,375 : INFO : PROGRESS: at 86.72% examples, 37387 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:47,423 : INFO : PROGRESS: at 88.36% examples, 37391 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:48,488 : INFO : PROGRESS: at 90.00% examples, 37370 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:49,554 : INFO : PROGRESS: at 91.54% examples, 37315 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:50,562 : INFO : PROGRESS: at 93.04% examples, 37292 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:51,581 : INFO : PROGRESS: at 94.72% examples, 37310 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:52,606 : INFO : PROGRESS: at 96.10% examples, 37231 words/s, in_qsize 2, out_qsize 0\n",
      "2017-11-28 19:51:53,607 : INFO : PROGRESS: at 97.64% examples, 37219 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:54,617 : INFO : PROGRESS: at 99.14% examples, 37195 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-28 19:51:55,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-28 19:51:55,186 : INFO : training on 8440800 raw words (2370696 effective words) took 63.8s, 37171 effective words/s\n",
      "2017-11-28 19:51:55,187 : INFO : saving Doc2Vec object under toy_data/model.bin, separately None\n",
      "2017-11-28 19:51:55,189 : INFO : not storing attribute syn0norm\n",
      "2017-11-28 19:51:55,191 : INFO : not storing attribute cum_table\n",
      "2017-11-28 19:51:55,523 : INFO : saved toy_data/model.bin\n"
     ]
    }
   ],
   "source": [
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, iter=train_epoch)# pretrained_emb=pretrained_emb, iter=train_epoch)\n",
    "\n",
    "#save model\n",
    "model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy_data/model.bin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "model=\"toy_data/model.bin\"\n",
    "test_docs=\"toy_data/test_docs.txt\"\n",
    "output_file=\"toy_data/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-28 19:54:23,473 : INFO : loading Doc2Vec object from toy_data/model.bin\n",
      "2017-11-28 19:54:23,720 : INFO : loading wv recursively from toy_data/model.bin.wv.* with mmap=None\n",
      "2017-11-28 19:54:23,721 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-28 19:54:23,721 : INFO : loading docvecs recursively from toy_data/model.bin.docvecs.* with mmap=None\n",
      "2017-11-28 19:54:23,722 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-28 19:54:23,723 : INFO : loaded toy_data/model.bin\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy_data/test_vectors.txt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
